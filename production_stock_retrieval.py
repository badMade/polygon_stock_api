#!/usr/bin/env python3
"""
FULL PRODUCTION SCRIPT FOR 6,628 TICKERS
Integrates with Polygon API and Notion database
"""

import json
import logging
import os
import time
from datetime import datetime, timedelta
from pathlib import Path

BASE_DATA_DIR = Path(os.getenv("STOCK_APP_DATA_DIR", Path(__file__).resolve().parent / "user-data"))
OUTPUT_DIR = BASE_DATA_DIR / "outputs"
UPLOADS_DIR = BASE_DATA_DIR / "uploads"

# Ensure logging output directory exists
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
UPLOADS_DIR.mkdir(parents=True, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUT_DIR / 'production_run.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ProductionStockRetriever:
    """Production system for bulk historical stock data retrieval.

    Retrieves stock data from the Polygon API for thousands of tickers across
    multiple 5-year time periods, batching requests to manage rate limits and
    saving results to JSON files for subsequent Notion database upload.

    Attributes:
        ticker_file: Path to JSON file containing ticker symbols.
        data_source_id: Notion database data source identifier.
        batch_size: Number of tickers to process per batch.
        processed: Count of tickers processed so far.
        saved: Count of records saved to batch files.
        failed: List of ticker symbols that failed processing.
        periods: List of 5-year time period dictionaries with from/to dates.
        tickers: List of ticker symbols loaded from ticker_file.
    """

    def __init__(self):
        # Using the actual 6,628 ticker file
        self.ticker_file = str(UPLOADS_DIR / "all_tickers.json")
        self.data_source_id = "7c5225aa-429b-4580-946e-ba5b1db2ca6d"
        self.batch_size = 100

        # Tracking
        self.processed = 0
        self.saved = 0
        self.failed = []
        self.tickers = []

        # 5-year chunks backwards from current
        self.periods = [
            {"from": "2020-01-01", "to": "2024-11-23", "label": "2020-2024"},
            {"from": "2015-01-01", "to": "2019-12-31", "label": "2015-2019"},
            {"from": "2010-01-01", "to": "2014-12-31", "label": "2010-2014"},
            {"from": "2005-01-01", "to": "2009-12-31", "label": "2005-2009"},
            {"from": "2000-01-01", "to": "2004-12-31", "label": "2000-2004"}
        ]

    def load_tickers(self):
        """Load ticker symbols from the configured JSON file.

        Reads the ticker file and populates the internal tickers list.

        Returns:
            int: Number of tickers loaded.

        Raises:
            FileNotFoundError: If the ticker file does not exist.
            json.JSONDecodeError: If the ticker file contains invalid JSON.
        """
        with open(self.ticker_file, 'r', encoding='utf-8') as f:
            self.tickers = json.load(f)
        return len(self.tickers)

    def get_polygon_data(self, ticker, period):
        """Retrieve stock aggregate data from the Polygon API.

        Fetches OHLCV (Open, High, Low, Close, Volume) data for a ticker
        within the specified time period. Currently returns placeholder data;
        replace with actual mcp_polygon.get_aggs calls in production.

        Args:
            ticker: Stock ticker symbol (e.g., "AAPL").
            period: Dictionary with "from", "to" date strings and "label".

        Returns:
            dict: Stock data containing:
                - ticker: The ticker symbol.
                - period: The period label.
                - has_data: Whether valid data was retrieved.
                - open, high, low, close: Price values or None.
                - volume: Trading volume or None.
                - vwap: Volume-weighted average price or None.
                - transactions: Transaction count or None.
                - data_points: Number of data points retrieved.
                - timespan: Data resolution ("minute", "hour", or "day").
        """
        # This is where you'd make the actual Polygon API call
        # Example structure:
        # response = mcp_polygon.get_aggs(
        #     ticker=ticker,
        #     multiplier=1,
        #     timespan="day",  # Try minute ‚Üí hour ‚Üí day
        #     from_=period["from"],
        #     to=period["to"],
        #     adjusted=True,
        #     sort="asc",
        #     limit=50000
        # )

        # For now, return a structured response
        # In production, parse the actual API response
        return {
            "ticker": ticker,
            "period": period["label"],
            "has_data": False,  # Set to True when data exists
            "open": None,
            "high": None,
            "low": None,
            "close": None,
            "volume": None,
            "vwap": None,
            "transactions": None,
            "data_points": 0,
            "timespan": "day"
        }

    def _create_base_properties(self, ticker, period, data, batch_num):
        """Create base Notion page properties for a ticker/period."""
        return {
            "Ticker": ticker,
            "Period": period["label"],
            "Has Data": "__YES__" if data["has_data"] else "__NO__",
            "Batch Number": batch_num,
            "date:Date:start": period["from"],
            "date:Date:is_datetime": 0,
            "date:Retrieved At:start": datetime.now().isoformat(),
            "date:Retrieved At:is_datetime": 1
        }

    def _add_numeric_properties(self, properties, data):
        """Add numeric data fields to properties if available."""
        for field in ["Open", "High", "Low", "Close", "Volume",
                      "VWAP", "Transactions", "Data Points"]:
            value = data.get(field.lower().replace(" ", "_"))
            if value is not None:
                properties[field] = value

        if data.get("timespan"):
            properties["Timespan"] = data["timespan"]

    def _process_ticker_period(self, ticker, period, batch_num):
        """Process a single ticker for one time period."""
        data = self.get_polygon_data(ticker, period)

        properties = self._create_base_properties(
            ticker, period, data, batch_num
        )

        if data["has_data"]:
            self._add_numeric_properties(properties, data)

        time.sleep(0.01)  # Rate limiting

        return {"properties": properties}

    def _log_progress(self, current_index, ticker):
        """Log progress update every 20 tickers."""
        if current_index % 20 == 0:
            pct = (self.processed / len(self.tickers)) * 100
            logger.info(
                "  ‚úì Progress: %d/%d (%.1f%%) - Current: %s",
                self.processed,
                len(self.tickers),
                pct,
                ticker,
            )

    def process_batch(self, batch, batch_num, total_batches):
        """Process a batch of tickers through all time periods.

        Iterates through each ticker in the batch, fetches data for all
        configured time periods, formats the results as Notion pages,
        and saves them to a batch file.

        Args:
            batch: List of ticker symbols to process.
            batch_num: Current batch number (1-indexed).
            total_batches: Total number of batches to process.

        Returns:
            int: Number of Notion page records created for this batch.
        """
        logger.info(
            "üìä BATCH %d/%d: Processing %d tickers",
            batch_num,
            total_batches,
            len(batch),
        )

        notion_pages = []

        for i, ticker in enumerate(batch, 1):
            for period in self.periods:
                page = self._process_ticker_period(ticker, period, batch_num)
                notion_pages.append(page)

            self.processed += 1
            self._log_progress(i, ticker)

        self.save_batch(notion_pages, batch_num)
        self.saved += len(notion_pages)

        return len(notion_pages)

    def save_batch(self, pages, batch_num):
        """Save batch data to a JSON file for later Notion upload.

        Writes the collected Notion page data along with metadata to a
        JSON file in the output directory.

        Args:
            pages: List of Notion page dictionaries to save.
            batch_num: Batch number used for file naming.
        """
        output_file = OUTPUT_DIR / f'batch_{batch_num:04d}_notion.json'

        batch_data = {
            "data_source_id": self.data_source_id,
            "batch_number": batch_num,
            "record_count": len(pages),
            "timestamp": datetime.now().isoformat(),
            "pages": pages
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, indent=2)

        logger.info("  üíæ Saved %d records to %s", len(pages), output_file)

    def create_notion_upload_script(self, total_batches):
        """Generate a Python script for uploading all batch files to Notion.

        Creates an executable Python script that iterates through all batch
        files and uploads them to the Notion database using the Notion API.

        Args:
            total_batches:
            Total number of batch files to include in the script.
        """
        script = f'''#!/usr/bin/env python3
"""
Upload all batch files to Notion database
Database: https://www.notion.so/638a8018f09d4e159d6d84536f411441
"""

import json
import time

# Configuration
DATA_SOURCE_ID = "{self.data_source_id}"
TOTAL_BATCHES = {total_batches}

def upload_batch(batch_num):
    """Upload a single batch to Notion"""
    filename = f'{OUTPUT_DIR}/batch_{{batch_num:04d}}_notion.json'

    with open(filename, 'r', encoding='utf-8') as f:
        batch_data = json.load(f)

    print(f"Uploading batch {{batch_num}}:
    {{batch_data['record_count']}} pages")

    # Split into chunks of 100 pages (Notion API limit)
    pages = batch_data['pages']
    for i in range(0, len(pages), 100):
        chunk = pages[i:i+100]

        # Use Notion API to create pages
        # notion.create_pages(
        #     parent={{"data_source_id": DATA_SOURCE_ID,
        # "type": "data_source_id"}},
        #     pages=chunk
        # )

        time.sleep(0.5)  # Rate limiting

    return batch_data['record_count']

# Upload all batches
total_uploaded = 0
for batch_num in range(1, TOTAL_BATCHES + 1):
    records = upload_batch(batch_num)
    total_uploaded += records
    print(f"Progress: {{batch_num}}/{{TOTAL_BATCHES}} batches,
    {{total_uploaded}} total records")

print(f"\\n‚úÖ Upload complete: {{total_uploaded}} records uploaded to Notion")
'''

        script_file = OUTPUT_DIR / 'notion_bulk_upload.py'
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script)

        os.chmod(script_file, 0o755)
        logger.info("üìù Upload script created: %s", script_file)

    def run(self):
        """Execute the full production data retrieval workflow.

        Orchestrates the complete retrieval process: loads tickers, divides
        them into batches, processes each batch through all time periods,
        saves checkpoints periodically, and generates a summary and upload
        script upon completion.

        Handles KeyboardInterrupt gracefully by saving progress before exit.

        Raises:
            Exception: Re-raises any unexpected errors after logging.
        """
        logger.info("=" * 80)
        logger.info("üöÄ PRODUCTION STOCK DATA RETRIEVAL - 6,628 TICKERS")
        logger.info("=" * 80)
        logger.info(
            "üìä Notion Database: https://www.notion.so/"
            "638a8018f09d4e159d6d84536f411441"
        )
        logger.info("üîó Data Source ID: %s", self.data_source_id)
        logger.info("=" * 80)

        start_time = datetime.now()

        try:
            # Load tickers
            ticker_count = self.load_tickers()
            logger.info("üìà Loaded %d tickers", ticker_count)

            # Calculate batches
            total_batches = (
                ticker_count + self.batch_size - 1) // self.batch_size
            total_est_records = ticker_count * len(self.periods)

            logger.info("üìä Configuration:")
            logger.info("  ‚Ä¢ Tickers: %d", ticker_count)
            logger.info("  ‚Ä¢ Batch size: %d", self.batch_size)
            logger.info("  ‚Ä¢ Total batches: %d", total_batches)
            logger.info(
                "  ‚Ä¢ Time periods: %d (5-year chunks)", len(self.periods))
            logger.info("  ‚Ä¢ Est. records: %d", total_est_records)
            logger.info(
                "  ‚Ä¢ Est. API calls: %d", ticker_count * len(self.periods))

            # Estimate time
            api_calls = ticker_count * len(self.periods)
            est_time_seconds = api_calls * 0.01  # 10ms per call
            est_time = timedelta(seconds=est_time_seconds)
            logger.info("  ‚Ä¢ Est. runtime: %s (plus Notion upload)", est_time)
            logger.info("=" * 80)

            # Process all batches
            for batch_num in range(1, total_batches + 1):
                start_idx = (batch_num - 1) * self.batch_size
                end_idx = min(start_idx + self.batch_size, ticker_count)
                batch = self.tickers[start_idx:end_idx]

                self.process_batch(batch, batch_num, total_batches)

                # Detailed progress every 10 batches
                if batch_num % 10 == 0 or batch_num == total_batches:
                    elapsed = datetime.now() - start_time
                    rate = (
                        self.processed / elapsed.total_seconds()
                        if elapsed.total_seconds() > 0 else 0
                    )
                    remaining_tickers = ticker_count - self.processed
                    eta = (
                        timedelta(seconds=remaining_tickers/rate)
                        if rate > 0 else timedelta(0)
                    )

                    logger.info("-" * 60)
                    logger.info(
                        "üìä CHECKPOINT - Batch %d/%d", batch_num, total_batches)
                    logger.info(
                        "  ‚Ä¢ Processed: %d/%d tickers", self.processed,
                        ticker_count
                        )
                    logger.info("  ‚Ä¢ Saved: %d records", self.saved)
                    logger.info("  ‚Ä¢ Rate: %.1f tickers/sec", rate)
                    logger.info("  ‚Ä¢ Elapsed: %s", elapsed)
                    logger.info("  ‚Ä¢ ETA: %s", eta)
                    logger.info("-" * 60)

                    # Save checkpoint
                    checkpoint = {
                        "batch": batch_num,
                        "processed": self.processed,
                        "saved": self.saved,
                        "timestamp": datetime.now().isoformat()
                    }
                    with open(
                        OUTPUT_DIR / 'checkpoint.json',
                            'w', encoding='utf-8') as f:
                        json.dump(checkpoint, f)

            # Generate upload script
            self.create_notion_upload_script(total_batches)

            # Final summary
            end_time = datetime.now()
            duration = end_time - start_time

            summary = {
                "execution": {
                    "status": "SUCCESS",
                    "start": start_time.isoformat(),
                    "end": end_time.isoformat(),
                    "duration": str(duration)
                },
                "results": {
                    "tickers_processed": self.processed,
                    "records_saved": self.saved,
                    "batches_created": total_batches,
                    "failed": len(self.failed)
                },
                "performance": {
                    "avg_ticker_time": (
                        duration.total_seconds() / self.processed
                        if self.processed > 0
                        else 0
                    ),
                    "tickers_per_second": (
                        self.processed / duration.total_seconds()
                        if duration.total_seconds() > 0
                        else 0
                    )
                },
                "notion": {
                    "database_url": (
                        "https://www.notion.so/"
                        "638a8018f09d4e159d6d84536f411441"
                    ),
                    "data_source_id": self.data_source_id
                }
            }

            summary_file = OUTPUT_DIR / 'production_summary.json'
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2)

            # Print results
            logger.info("=" * 80)
            logger.info("‚úÖ PRODUCTION RUN COMPLETE")
            logger.info("=" * 80)
            logger.info("üìä Results:")
            logger.info("  ‚Ä¢ Processed: %d tickers", self.processed)
            logger.info("  ‚Ä¢ Saved: %d records", self.saved)
            logger.info("  ‚Ä¢ Duration: %s", duration)
            logger.info("  ‚Ä¢ Files: %d batch files created", total_batches)
            logger.info("=" * 80)
            logger.info("üìã Next Steps:")
            logger.info(
                "  1. Review batch files in %s/", OUTPUT_DIR
                )
            logger.info("  2. Run notion_bulk_upload.py to upload to Notion")
            logger.info(
                "  3. Monitor at: https://www.notion.so/"
                "638a8018f09d4e159d6d84536f411441"
                )
            logger.info("=" * 80)

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è Run interrupted - progress saved")
            logger.info(
                "Processed %d tickers before interruption", self.processed
                )

        except Exception as e:  # pylint: disable=broad-except
            logger.error("‚ùå Error: %s", e)
            raise


if __name__ == "__main__":
    retriever = ProductionStockRetriever()
    retriever.run()
