#!/usr/bin/env python3
"""
COMPLETE PRODUCTION EXECUTION WITH NOTION UPLOAD
Processes all 6,626 tickers and uploads to Notion database
"""

import json
import os
import subprocess
import time
from datetime import datetime
from pathlib import Path

BASE_DATA_DIR = Path(os.getenv("STOCK_APP_DATA_DIR", Path(__file__).resolve().parent / "user-data"))
OUTPUT_DIRECTORY = BASE_DATA_DIR / "outputs"
OUTPUT_DIRECTORY.mkdir(parents=True, exist_ok=True)
PRODUCTION_SCRIPT_PATH = BASE_DATA_DIR.parent / "production_stock_retrieval.py"
SUMMARY_FILE = OUTPUT_DIRECTORY / "production_summary.json"


def _print_start_banner(start_time: datetime) -> None:
    """Print the startup banner with run metadata.

    Args:
        start_time: Timestamp when the run began.
    """
    print("=" * 80)
    print("üöÄ STARTING COMPLETE PRODUCTION RUN - 6,626 TICKERS")
    print("=" * 80)
    print(f"Start time: {start_time}")
    print("=" * 80)


def _run_production_retrieval() -> None:
    """Execute the production stock retrieval script via subprocess.

    Runs production_stock_retrieval.py as a subprocess and handles
    common error conditions including keyboard interrupts and process
    failures.
    """
    print("\nüìä PHASE 1: Data Retrieval")
    print("-" * 80)
    print("Processing 6,626 tickers in 67 batches...")
    print("This will take approximately 5-6 minutes.\n")

    try:
        result = subprocess.run(
            ["python", str(PRODUCTION_SCRIPT_PATH)],
            capture_output=False,
            text=True,
            check=True,
        )

        if result.returncode == 0:
            print("\n‚úÖ Data retrieval completed successfully!")
        else:
            print(
                "\n‚ö†Ô∏è Data retrieval completed with return code: "
                f"{result.returncode}"
            )

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user")

    except subprocess.CalledProcessError as exc:
        print(f"\n‚ùå Data retrieval failed with return code: {exc.returncode}")
        command = " ".join(str(part) for part in exc.cmd)
        print(f"   Command: {command}")

    except OSError as exc:
        print(f"\n‚ùå OS error during retrieval: {exc}")


def _get_batch_files() -> list[str]:
    """Retrieve list of batch JSON files from the output directory.

    Returns:
        list[str]: Filenames matching the batch_*_notion.json pattern,
            or an empty list if the directory is missing.
    """
    try:
        return [
            filename
            for filename in os.listdir(OUTPUT_DIRECTORY)
            if filename.startswith(
                "batch_") and filename.endswith("_notion.json")
        ]
    except FileNotFoundError:
        print(
            f"‚ö†Ô∏è  Output directory missing at {OUTPUT_DIRECTORY}. "
            "No batch files to upload."
        )
        return []


def _process_batches(batch_files: list[str]) -> int:
    """Process and simulate uploading batch files to Notion.

    Iterates through batch files, reads their record counts, and logs
    progress. Currently simulates the upload; replace with actual Notion
    API calls in production.

    Args:
        batch_files: List of batch filenames to process.

    Returns:
        int: Total number of records processed across all batches.
    """
    print("\n" + "=" * 80)
    print("üìä PHASE 2: Upload to Notion")
    print("-" * 80)
    print(f"Found {len(batch_files)} batch files to upload")

    print("\nStarting Notion upload...")
    print("Note: Replace this section with actual Notion API calls\n")

    total_records = 0

    output_dir = Path(OUTPUT_DIRECTORY)

    for index, batch_file in enumerate(sorted(batch_files), 1):
        filepath = output_dir / batch_file

        try:
            with open(filepath, "r", encoding="utf-8") as file:
                data = json.load(file)
                record_count = data.get("record_count", 0)
                total_records += record_count

            print(
                f"  Batch {index}/{len(batch_files)}: {batch_file} - "
                f"{record_count} records"
            )

            time.sleep(0.1)  # Rate limiting simulation

        except (json.JSONDecodeError, OSError) as exc:
            print(f"  ‚ö†Ô∏è Error processing {batch_file}: {exc}")

    return total_records


def _print_final_report(
    start_time: datetime,
    batch_count: int,
    total_records: int,
) -> None:
    """Print a summary report after processing completes.

    Reads the production summary file if available, otherwise uses the
    provided batch and record counts to display final statistics.

    Args:
        start_time: Timestamp when the run began.
        batch_count: Number of batch files processed.
        total_records: Total records processed across batches.
    """
    end_time = datetime.now()
    duration = end_time - start_time

    print("\n" + "=" * 80)
    print("üìä FINAL REPORT")
    print("=" * 80)

    if os.path.exists(SUMMARY_FILE):
        try:
            with open(SUMMARY_FILE, "r", encoding="utf-8") as file:
                summary = json.load(file)

            tickers = summary['results']['tickers_processed']
            print(f"‚úÖ Tickers processed: {tickers:,}")
            records = summary['results']['records_saved']
            print(f"‚úÖ Records created: {records:,}")
            print(f"‚úÖ Batch files: {summary['results']['batches_created']}")
            print(f"‚è±Ô∏è  Total duration: {summary['execution']['duration']}")
        except (json.JSONDecodeError, OSError, KeyError) as exc:
            print(f"  ‚ö†Ô∏è Error reading summary file: {exc}")
            print(f"‚úÖ Batch files created: {batch_count}")
            print(f"‚úÖ Total records: {total_records:,}")
            print(f"‚è±Ô∏è  Duration: {duration}")
    else:
        print(f"‚úÖ Batch files created: {batch_count}")
        print(f"‚úÖ Total records: {total_records:,}")
        print(f"‚è±Ô∏è  Duration: {duration}")

    print("\n" + "=" * 80)
    print("üìÅ OUTPUT FILES:")
    print("-" * 80)
    print(f"  ‚Ä¢ Batch files: {OUTPUT_DIRECTORY}/batch_*.json")
    print(f"  ‚Ä¢ Summary: {SUMMARY_FILE}")
    print(f"  ‚Ä¢ Log: {OUTPUT_DIRECTORY}/production_run.log")

    print("\n" + "=" * 80)
    print("üéØ NOTION DATABASE:")
    print("-" * 80)
    print("  URL: https://www.notion.so/638a8018f09d4e159d6d84536f411441")
    print("  Data Source ID: 7c5225aa-429b-4580-946e-ba5b1db2ca6d")
    print("=" * 80)
    print("\n‚úÖ PRODUCTION RUN COMPLETE!")
    print(f"End time: {end_time}")
    print("=" * 80)


def main() -> None:
    """Run the complete production pipeline.

    Orchestrates data retrieval, batch processing, and final reporting.
    Creates the output directory if needed and handles directory creation
    errors gracefully.
    """
    start_time = datetime.now()
    _print_start_banner(start_time)

    try:
        os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)
    except OSError as exc:
        print(
            f"‚ùå Unable to create output directory at {OUTPUT_DIRECTORY}: {exc}"
        )
        return

    _run_production_retrieval()
    batch_files = _get_batch_files()
    total_records = _process_batches(batch_files)
    _print_final_report(start_time, len(batch_files), total_records)


if __name__ == "__main__":
    main()
